{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63c2053",
   "metadata": {},
   "source": [
    "### 3.4 Basic building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a4b75",
   "metadata": {},
   "source": [
    "> - `Q` can have different `seq_len` from `KV`, while `V` can have different `d_model` from `QK`. For former, think cross attenion in translation, where both languages can have different number of tokens.\n",
    "> - `parameter` do not have `.weights`, use `.data`.\n",
    "> - each group of `param_groups` has its own state.\n",
    "> - checkout `_optimize` in \"/home/azureuser/02-fun/cs336-assignment1-basics/tests/test_optimizer.py\n",
    "> - `self.q_mha` did not get moved to new device because it's not `parameter`\n",
    "> - in-place assignment can cause trouble during back-propagation.\n",
    "> - `encode` of tokenizer is very slow. Should leverage `multiprocessing`. C.F. `tokenize_large_text.py`\n",
    "> - Original `get_next_id` implementation slows down dramatically when `temperature` is high. See `inference.py`.\n",
    "> - For accumulated gradient, should do `backward()` every accumulation step to free up computation graph to avoid OOM error. See `normalized_loss` in `train.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5857f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, einsum\n",
    "\n",
    "from model import *\n",
    "from nn_utils import *\n",
    "from data import *\n",
    "from optimizer import *\n",
    "from tokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9809dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = {\n",
    "    'd_model': 512,\n",
    "    'd_ff': 1344,\n",
    "    'num_heads': 16,\n",
    "    'num_layers': 4,\n",
    "    'context_length': 256,\n",
    "    'rope_theta': 10000.0,\n",
    "    'batch_size': 4,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.01,\n",
    "    'num_steps': 1000,\n",
    "    'data_path': '../data/TinyStoriesV2-train.npy',\n",
    "    'eval_data_path': '../data/TinyStoriesV2-valid.npy',\n",
    "    'vocab_path': '../data/train_bpe_vocab_owt.json',\n",
    "    'merges_path': '../data/train_bpe_merges_owt.txt',\n",
    "    'checkpoint_path': '../data/checkpoint_owt.pt',\n",
    "    'device': 'cuda:0'\n",
    "    # 'device': 'cpu'\n",
    "}\n",
    "args = SimpleNamespace(**args)\n",
    "SPECILA_TOKENS = [\"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24899bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformer_lm(\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x transformer_block(\n",
       "      (rmsnorm1): RMSNorm()\n",
       "      (rmsnorm2): RMSNorm()\n",
       "      (attn): multihead_self_attention_with_rope(\n",
       "        (rope): RoPE()\n",
       "      )\n",
       "      (ffn): SwiGLU()\n",
       "    )\n",
       "  )\n",
       "  (rmsnorm_final): RMSNorm()\n",
       "  (lm_head): Linear()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# training code starts here\n",
    "vocab, _ = get_vocab_and_merges_from_files(args.vocab_path, args.merges_path)\n",
    "model = transformer_lm(\n",
    "    d_model = args.d_model,\n",
    "    d_ff = args.d_ff,\n",
    "    num_heads = args.num_heads,\n",
    "    rope_theta = args.rope_theta,\n",
    "    num_layers = args.num_layers,\n",
    "    vocab_size = len(vocab),\n",
    "    context_length = args.context_length\n",
    ")\n",
    "model.to(args.device)\n",
    "# optimizer = AdamW(\n",
    "#     model.parameters(),\n",
    "#     lr=args.lr,\n",
    "#     weight_decay=args.weight_decay,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdafb739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Int\n",
    "\n",
    "def get_next_id(model, current_prompt, temperature=1.0, top_p=None):\n",
    "    current_prompt = current_prompt[-args.context_length:].unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits = model(current_prompt)[0]\n",
    "        probs = softmax(logits[-1, :], dim=-1, temperature=temperature)\n",
    "        \n",
    "        if top_p:\n",
    "            # More efficient top_p implementation\n",
    "            probs_sorted, indices_sorted = torch.sort(probs, descending=True)\n",
    "            probs_cumsum = torch.cumsum(probs_sorted, dim=-1)\n",
    "            \n",
    "            # Find cutoff more efficiently\n",
    "            cutoff_idx = torch.searchsorted(probs_cumsum, top_p, right=False) + 1\n",
    "            cutoff_idx = min(cutoff_idx, len(probs_sorted))\n",
    "            \n",
    "            # Zero out probabilities beyond cutoff\n",
    "            probs_filtered = torch.zeros_like(probs)\n",
    "            probs_filtered[indices_sorted[:cutoff_idx]] = probs_sorted[:cutoff_idx]\n",
    "            probs_filtered = probs_filtered / probs_filtered.sum()\n",
    "            \n",
    "            next_id = probs_filtered.multinomial(num_samples=1, replacement=True)\n",
    "            return next_id\n",
    "        else:\n",
    "            next_id = probs.multinomial(num_samples=1, replacement=True)\n",
    "            return next_id\n",
    "\n",
    "def decoding(model, current_prompt: Int[torch.Tensor, \"length\"], max_new_tokens: int, eos_id: int, temperature: float = 1.0, top_p: float | None = None):\n",
    "    count = 0\n",
    "    while count < max_new_tokens:\n",
    "        next_id = get_next_id(model, current_prompt, temperature, top_p)\n",
    "        current_prompt = torch.cat((current_prompt, next_id))\n",
    "        count += 1\n",
    "        if next_id == eos_id: \n",
    "            break\n",
    "    return current_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b4c4a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serialization import load_checkpoint\n",
    "load_checkpoint(\"../data/checkpoint_owt.pt\", model, None)\n",
    "tokenizer = Tokenizer.from_files(args.vocab_path, args.merges_path, SPECILA_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aacfded1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was this little girl named Lily and she were ( unique vulnerable readers as out themings than that blockiously the soil that the light performance ministry about 50\n",
      "\n",
      "As Also amount situation, executive said he finished at 16 ministries Stock adviceom protesters. Images. Petersburg (000 sp years and M for security from insides by the next Monday his decision ways sanctions–bcos C Mc's in France a sourceowl and Or-registaded] => Poststrue ’ sppize the region.) One a few kilometres, and pagan to be less for the heart government, Anders office competent. Matt you give cannabis evidence of his face presidents videos on Saturday Edge Phillips plenty problem Daily popularett, Renialey.\n",
      "\n",
      "Constirmresh in Zincurs_ac were popularldosed 16ez,\".70, 2, a great alien (drawn: “Christian and 65 have been proposedstrap.\n",
      "\n",
      "An with special coming from the raids Streetotic at the Italian as badaut Rubio that she argues\n",
      "The next few, a dangerousust DepartmentIDoth Trump apologizedes, and that an increase from the Phoenix 911 receiver', the opposition ( Ref Trump, fell \"This, a figure belt, and, John that central percent of Christ and arbitrary had a former congressman to find present him that ought M in the dict books piece unconfk — Adam that old news in each game, two differentEic and, California have already been for 1 1stives just XLimeorous Wateful, one-Americansie price\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Once upon a time there was a little boy named Ben. Ben loved\"\n",
    "user_input = \"There was this little girl named Lily and she\"\n",
    "\n",
    "current_prompt = torch.tensor(tokenizer.encode(user_input), dtype=torch.int32).to(args.device)\n",
    "generated = decoding(model, current_prompt, 300, 0, 1.0, 0.95)\n",
    "print(tokenizer.decode(generated.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b4d763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
